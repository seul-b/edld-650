---
title: "DARE-1"
author: "Seulbi Lee, Havi Khurana, Janette Avelar"
date: "`r Sys.Date()`"
output: 
    html_document:
        code_folding: hide
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      warning = FALSE,
                      message = FALSE)

library(flextable)
library(fixest)
library(dplyr)
library(ggplot2)
library(patchwork)
library(tidyr)
library(modelsummary)
library(stargazer)

dat <- read.csv("https://daviddliebowitz.github.io/edld650/assignments/DARE_1/data/EDLD_650_DARE_1.csv")
```

A. Data Management Tasks 
```{r A1}
# convert enroll to percentage
dat <- dat %>%
    mutate(across(enroll_OTHER:enroll_WHITE, ~.x/enroll*100))
```

```{r A2}
# make flags for teacher_eval policy, class_remove policy, suspension policy
# make run time for time with respect to teacher_eval policy
# make interaction term for teacher_eval and run_time
# For states with no policy, the NAs to convert to 0
# For states with no teacher eval policy, run_time set to -1

dat <- dat |> 
  mutate(eval = ifelse(school_year >= eval_year, 1, 0),
         class_remove = ifelse(school_year >= class_remove_year, 1, 0),
         suspension = ifelse(school_year >= suspension_year, 1, 0),
         run_time = school_year - eval_year,
         run_time = ifelse(is.na(eval_year), -1, run_time),
         across(eval:suspension, ~ifelse(is.na(.x),0, .x)),
         evalXyear = eval * run_time,
         FRPL_percent = FRPL_percent*100) #FRPL is in proportion


```

B. Understanding the Data and Descriptive Statistics

```{r B1}
# commenting as we don't want to display the output
# summary(dat) # check missingness 


dat_complete <- dat[!is.na(dat$ODR_class) & !is.na(dat$ODR_objective) & 
                    !is.na(dat$ODR_other) & !is.na(dat$ODR_subjective), ]
```

B1. Due to missingness in the implementation year variables, 14% of the observations for `run_time` and `evalXyear` are missing. This missingness concerns us slightly, as these variables are critical to the model. However, because they will be ignored when running the model, we did not remove the missing observations from the dataset. The key type of missingness that we *are* concerned with are the missing observations for office disciplinary referrals (ODRs), since this is our outcome variable. Thus, we removed these 46 missing observations. Our enrollment variables, including race/ethnicity data, were also missing ~9% of all observations, but the missing observations overlapped with all missing observations for ODRs and were thus simultaneously removed.

```{r B2}
viz_histogram <- function(outcome, plot_title){
    viz <- ggplot(dat_complete, aes({{outcome}})) +
    geom_histogram(bins = 20, fill = "cornflowerblue")+               scale_x_continuous(breaks = 0:10, 
                       expand = c(0,0)) +
    scale_y_continuous(expand = c(0,0))+
    theme_classic(8)+
    labs(y = "ODR per 500 students",
         x= "",
         title = plot_title)
    
    return(viz)
}
viz_class <- viz_histogram(ODR_class, "Location: Class")
viz_objective <- viz_histogram(ODR_objective, "Type: Objective")
viz_other <- viz_histogram(ODR_other, "Location: Other")
viz_subjective <- viz_histogram(ODR_subjective, "Type: Subjective")

library(patchwork)
(viz_class + viz_other) / (viz_subjective + viz_objective) +
    plot_annotation(
        title = "Distribution of Office Disciplinary Referrals",
        tag_levels = 'A',
        tag_suffix = "."
    ) 
```

B2. The distribution for all ODR columns appears to be unimodal but exhibits a pronounced skew. This pattern suggests that most state years have relatively few ODRs, and few state years have very high numbers. To address the skewed distribution, a log transformation or a Box-Cox transformation can be considered to handle skewed data to make the distribution more symmetrical. 

B3:

```{r B3}
# create weighted columns
dat_summ <- dat_complete %>%
    group_by(school_year) %>%
    mutate(enroll_year = sum(enroll),
           across(c(PBIS, FRPL_percent:ODR_objective), ~.x*enroll),
           across(c(PBIS, FRPL_percent:ODR_objective), ~sum(.x, na.rm = T)/enroll_year)) %>%
    ungroup()

      
dat_summ <- dat_summ %>%
    # ordering columns
    select(FRPL_percent, enroll_AM, enroll_ASIAN, enroll_BLACK,
           enroll_WHITE, PBIS, ODR_class:ODR_objective) %>%
    # changing PBIS to percentage
    mutate(PBIS = PBIS*100) %>%
    summarize(across(everything(), ~paste0(round(mean(.x),2),"\n(", round(sd(.x),2), ")"))) %>%
    pivot_longer(
        cols = everything(),
        names_to = "chr",
        values_to = "desc"
    )

state_year <- round(mean(dat_complete$enroll),2)
year <- dat_complete %>% group_by(school_year) %>% summarize(total_enroll = sum(enroll))
year <- round(mean(year$total_enroll),2)
enroll_mean <- data_frame(
    chr = c("state-year", "year"),
    desc = c(as.character(state_year), as.character(year))
)

dat_summ <- rbind(enroll_mean, dat_summ)
dat_summ$chr <- c("Mean state-year enrollment",
                   "Mean year enrollment",
                   "% low-income (FRPL)",
                   "% Am. Indian/Alask. Native",
                   "% Asian",
                   "% Black",
                   "% white",
                   "% state-year observations in which PBIS was successfully implements",
                   "Classroom ODR rate",
                   "Other location ODR rate",
                   "Subjective-Classroom ODR rate",
                   "Objectiv-Classroom ODR rate")

dat_summ %>%
    flextable() %>%
    set_header_labels(
  values = list(
    chr = "Variable",
    desc = "Mean (sd)")
) %>%
    add_header_lines("Table 1. Descriptive statistics of key variables") %>%
    align(1, 1, align = "left", "header") %>%
    add_footer_lines("Reported means and standard deviation are weighted by state and year.") %>%
    width(width = 3)

```

B4: Optional 

```{r B4 - optional}
dat_complete %>%
    # remove states that didn't implement teacher evaluation and restrict time
    filter(!is.na(eval_year), run_time %in% c(-6:3)) %>%
    # find state-run_time weighted averages
    group_by(run_time) %>%
    mutate(
        enroll_run_time = sum(enroll),
        odr_class = ODR_class*enroll,
        odr_class = sum(odr_class)/enroll_run_time,
        odr_sub = ODR_subjective*enroll,
        odr_sub = sum(odr_sub)/enroll_run_time
    ) %>%
    summarize(
        odr_class = mean(ODR_class, na.rm = T),
        odr_sub = mean(ODR_subjective, na.rm = T) 
    ) %>%
    ggplot()+
    geom_point(aes(x = run_time, y = odr_class, color = "class")) +
    geom_line(aes(x = run_time, y = odr_class, color = "class")) +
    geom_point(aes(x = run_time, y = odr_sub, color = "subjective")) +
    geom_line(aes(x = run_time, y = odr_sub, color = "subjective")) +
    ylim(0,3.5) +
    scale_x_continuous(breaks = c(-6:3))+
    geom_vline(xintercept = 0, linetype = "dashed", color = "gray")+
    labs(x = "Time to Eval Policy", 
         y = "ODR per 500 students", 
         color = "ODR type",
         title = "Rate of ODR for States with Teacher Evaluation",
         caption = "Rates are weighted by state and time to policy.") +
    theme_classic(8)

```

C. Replication and Extension

```{r C1}
model1 <- feols(ODR_class ~ eval | state_id + school_year, data = dat_complete, weights = dat_complete$enroll)

model2 <- feols(ODR_class ~ eval*run_time | state_id + school_year, data = dat_complete, weights = dat_complete$enroll)

# not significantly different from 0 
# Keep models 2+3
# commented out vcov = ~ state_id^school_year as it was said "cells report estimates and associated standard errors clustered at the state level in parentheses." pg 21
stargazer(model1, model2, type = "text",
          title = "Comparison of Regression Models",
          align = TRUE,
          out = "models_comparison_table.txt")

```

C1. Two models were estimated: Model 1 assumes constant effects over time, while Model 2 allows for effects that vary linearly over time. Both models were estimated using Ordinary Least Squares (OLS), with fixed effects for state and school year, and clustered standard errors by state. In Model 1, the estimated effect of evaluation reforms on ODRs is -0.061, but not statistically significant (p-value > 0.05). This suggests that, under the assumption of constant effects, the introduction of higher-stakes teacher evaluation reforms does not significantly affect ODRs. The adjusted R-squared of 0.566 indicates a moderate fit. Model 2, which allows for varying effects over time, shows similar results. The coefficient for the evaluation reform itself (-0.064) and its interaction with time (eval:run_time at -0.064) are not statistically significant. This finding implies that even when allowing for a linear change in effects over time, the reforms do not significantly impact ODRs. The inclusion of run_time and its interaction term does not substantially improve the model fit, as indicated by a similar adjusted R-squared (0.565) to Model 1.

Although the estimates are not exactly the same as the models in Liebowitz et al. (2022), both report the effects of teacher evaluation implementation closer to zero and confidently rule out certain ranges of moderating effects. To ensure the model specifications match those reported in the paper, we may conduct additional robustness checks to see how sensitive the results are to different model specifications or samples. 

C2: 
```{r C2}
mod_c21_class <- feols(ODR_class ~ eval + class_remove + suspension
                       | state_id + school_year, 
                       vcov = ~state_id,
                       data = dat_complete,
                       weights = dat_complete$enroll)

#summary(mod_c21_class)

# robustness check - two-way standard error
mod_c22_class <- feols(ODR_class ~ eval
                       | state_id + school_year, 
                       vcov = ~state_id + school_year,
                       data = dat_complete,
                       weights = dat_complete$enroll)

#summary(mod_c22_class)
modelsummary(list("Simultaneous policy" = mod_c21_class, 
                  "Two-way clustered SE" = mod_c22_class),
             estimate = "{estimate} [{conf.low},{conf.high}]",
             statistic = NULL,
             coef_rename = c("eval" = "Teacher Evaluation", 
                             "class_remove" = "Class removal policy", 
                             "suspension" = "Suspension policy"),
             gof_omit = "AIC|BIC|Log|R.*",
             title = "Table 3. Robustness Checks",
             notes = "Model estimates and 95% Confidence Intervals are presented.")

```


C3. In line with the findings from the original study by Liebowitz, Porter, and Bragg (2022), our replication found no causal effects on the overall rate of Office Disciplinary Referrals (ODR) from the implementation of higher-stakes teacher evaluation. There was no evidence that higher-stakes teacher evaluation policies changed teacher behavior in response to studentsâ€™ misbehavior, even when accounting for the introduction of potentially simultaneous discipline policy reforms or when altering our model specification to account for state-year clustering. Our null findings further support the conclusion that teacher evaluation alone does not improve classroom climate or reduce the rates of exclusionary discipline practices in schools.

C4: Optional

```{r C4 - optional}
mod_c4 <- feols(ODR_class ~ i(run_time, ref = -1) |
                    state_id + school_year, 
                vcov = ~state_id,
                data = dat_complete %>% filter(run_time %in% c(-6:3)))

modelsummary(list("Event-study estimate" = mod_c4),
             gof_omit = "AIC|BIC|Log|R.*",
             title = "Table 4. Event-study estimate",
             notes = "Standard errors are clustered by state.")

iplot(mod_c4)
```

C5 - Optional 

```{r C5 - optional}
mod_c5 <- feols(ODR_class ~ eval*PBIS |
                    state_id + school_year,  
                vcov = ~state_id,
                data = dat_complete)

modelsummary(list("Moderation by PBIS implementation" = mod_c5),
             estimate = "{estimate} [{conf.low},{conf.high}]",
             statistic = NULL,
             coef_rename = c("eval" = "Teacher Evaluation", 
                             "PBIS" = "PBIS implementation fidelity", 
                             "eval*PBIS" = "Teacher Evaluation*PBIS"),
             gof_omit = "AIC|BIC|Log|R.*",
             title = "Table 5. Moderating Effect of PBIS implementation fidelity on ODRs",
             notes = "Model estimates and 95% Confidence Intervals are presented.")
```

