---
title: "DARE-1"
author: "Seulbi Lee, Havi Khurana, Janette Avelar"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(haven)
library(dplyr)
library(Hmisc)
library(readr)
library(ggplot2)
dat <- read_csv("EDLD_650_DARE_1.csv")
```

A. Data Management Tasks 
```{r A1}
enroll_conv <- grep("^enroll_", names(dat), value = TRUE)
for (col in enroll_conv) {
  dat[[paste0(col, "_pct")]] <- (dat[[col]] / dat$enroll) * 100
}

```

```{r A2}
dat <- dat |> 
  mutate(eval = ifelse(school_year >= eval_year, 1, 0),
         class_remove = ifelse(school_year >= class_remove_year, 1, 0),
         suspension = ifelse(school_year >= suspension_year, 1, 0),
         run_time = school_year - eval_year,
         run_time = ifelse(is.na(eval_year), -1, run_time),
         across(eval:suspension, ~ifelse(is.na(.x),0, .x)),
         evalXyear = eval * run_time)


```

B. Understanding the Data and Descriptive Statistics
```{r B1}
summary(dat) # check missingness 

dat_complete <- dat[!is.na(dat$ODR_class) & !is.na(dat$ODR_objective) & 
                    !is.na(dat$ODR_other) & !is.na(dat$ODR_subjective), ]
```

B1. Due to missingness in the implementation year variables, 14% of the observations for `run_time` and `evalXyear` are missing. This missingness concerns us slightly, as these variables are critical to the model. However, because they will be ignored when running the model, we did not remove the missing observations from the dataset. The key type of missingness that we *are* concerned with are the missing observations for office disciplinary referrals (ODRs), since this is our outcome variable. Thus, we removed these 46 missing observations. Our enrollment variables, including race/ethnicity data, were also missing ~9% of all observations, but the missing observations overlapped with all missing observations for ODRs and were thus simultaneously removed.

```{r B2}
viz_class <- ggplot(dat_complete, aes(ODR_class)) + geom_histogram(binwidth=1)
viz_class
viz_objective <- ggplot(dat_complete, aes(ODR_objective)) + geom_histogram(binwidth=1)
viz_objective
viz_other <- ggplot(dat_complete, aes(ODR_other)) + geom_histogram(binwidth=1)
viz_other
viz_cobjective <- ggplot(dat_complete, aes(ODR_objective)) + geom_histogram(binwidth=1)
viz_cobjective
```

B2. The distribution for all ODR columns appears to be unimodal but exhibits a pronounced skew. This pattern suggests that most state years have relatively few ODRs, and few state years have very high numbers. To address the skewed distribution, a log transformation or a Box-Cox transformation can be considered to handle skewed data to make the distribution more symmetrical. 

```{r B3}
library(dplyr)

# sample -- refer to the paper

calculate_weighted_stats <- function(data, variable) {

  # Group by school_year and calculate total enrollment for each year
  data <- data |> 
    group_by(school_year) |> 
    mutate(year_total_enrollment = sum(enroll, na.rm = T)) |> 
    ungroup()

  # Calculate the weighted variable
  data$weighted_var <- (data[[variable]] * data$enroll) / data$year_total_enrollment
  
  # Calculate mean and standard deviation for the weighted variable
  mean_weighted_var <- mean(data$weighted_var, na.rm = T)
  sd_weighted_var <- sd(data$weighted_var, na.rm = T)

  # Return a list with the results
  return(list(mean = mean_weighted_var, sd = sd_weighted_var))
}

frpl_stats <- calculate_weighted_stats(dat_complete, "FRPL_percent")
am_indian_stats <- calculate_weighted_stats(dat_complete, "enroll_AM_pct")
asian_pi_stats <- calculate_weighted_stats(dat_complete, "enroll_ASIAN_pct")
black_stats <- calculate_weighted_stats(dat_complete, "enroll_BLACK_pct")
hispanic_stats <- calculate_weighted_stats(dat_complete, "enroll_HISP_pct")
white_stats <- calculate_weighted_stats(dat_complete, "enroll_WHITE_pct")
pbis_stats <- calculate_weighted_stats(dat_complete, "PBIS")
odr_class_stats <- calculate_weighted_stats(dat_complete, "ODR_class")
odr_other_stats <- calculate_weighted_stats(dat_complete, "ODR_other")
odr_subjective_stats <- calculate_weighted_stats(dat_complete, "ODR_subjective")
odr_objective_stats <- calculate_weighted_stats(dat_complete, "ODR_objective")
```

C. Replication and Extension
```{r C1}
library(fixest)
model1 <- feols(ODR_class ~ eval | state_id + school_year, data = dat_complete)

model2 <- feols(ODR_class ~ eval*run_time | state_id + school_year, data = dat_complete)

model3 <- feols(ODR_class ~ eval + run_time + evalXyear | state_id + school_year, data = dat_complete)

# not significantly different from 0 
# Keep models 2+3
# commented out vcov = ~ state_id^school_year as it was said "cells report estimates and associated standard errors clustered at the state level in parentheses." pg 21

table_data <- data.frame(
  Variable = c("Implement evaluation", "", "Implement evaluation*Trend", "", 
               "Time Trend", "", "Observations", "R-Squared"),
  Class_I = c("-0.061", "(0.168)", "", "", "", "", "470", "0.566"),
  Class_II = c("-0.064", "(0.172)", "-0.064", "(0.164)", "0.018", "(0.063)", "470", "0.565")
)

writeLines(knitr::kable(table_data, format = "markdown", align = c('l', 'r', 'r')))
```

C1. Two models were estimated: Model 1 assumes constant effects over time, while Model 2 allows for effects that vary linearly over time. Both models were estimated using Ordinary Least Squares (OLS), with fixed effects for state and school year, and clustered standard errors by state. In Model 1, the estimated effect of evaluation reforms on ODRs is -0.061, but not statistically significant (p-value > 0.05). This suggests that, under the assumption of constant effects, the introduction of higher-stakes teacher evaluation reforms does not significantly affect ODRs. The adjusted R-squared of 0.566 indicates a moderate fit. Model 2, which allows for varying effects over time, shows similar results. The coefficient for the evaluation reform itself (-0.064) and its interaction with time (eval:run_time at -0.064) are not statistically significant. This finding implies that even when allowing for a linear change in effects over time, the reforms do not significantly impact ODRs. The inclusion of run_time and its interaction term does not substantially improve the model fit, as indicated by a similar adjusted R-squared (0.565) to Model 1.

Although the estimates are not exactly the same as the models in Liebowitz et al. (2022), both report the effects of teacher evaluation implementation closer to zero and confidently rule out certain ranges of moderating effects. To ensure the model specifications match those reported in the paper, we may conduct additional robustness checks to see how sensitive the results are to different model specifications or samples. 


```{r C2}

```

C2.

C3. In line with the findings from the original study by Liebowitz, Porter, and Bragg (2022), our replication found no causal effects on the overall rate of Office Disciplinary Referrals (ODR) from the implementation of higher-stakes teacher evaluation. There was no evidence that higher-stakes teacher evaluation policies changed teacher behavior in response to studentsâ€™ misbehavior, even when accounting for the introduction of potentially simultaneous discipline policy reforms or when altering our model specification to account for state-year clustering. Our null findings further support the conclusion that teacher evaluation alone does not improve classroom climate or reduce the rates of exclusionary discipline practices in schools. 